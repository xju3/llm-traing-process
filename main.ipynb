{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U transformers\n",
    "! pip install -U torch\n",
    "! pip install -U accelerate\n",
    "! pip install -U datasets\n",
    "! pip install huggingface_hub\n",
    "! pip install fastapi uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "0.Install Required Libraries:\n",
    "Install Hugging Face libraries, along with either PyTorch or TensorFlow, depending on your preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Step 1: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset from Hugging Face. Here, we use \"wikitext\" as an example.\n",
    "# dataset = load_dataset(\"wikitext\", \"LAW_benchmark\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"validation\"]  # Use a validation split as the eval dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Step 2: Load the tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"  # Change to your model's name if different\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token ID if it's missing (especially for models like GPT-2)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as padding if no pad token exists\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id  # Update model config accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Step 3: Define the tokenization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize and set 'labels' equal to 'input_ids' for language modeling\n",
    "    tokens = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()  # Setting labels for causal language modeling\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization to the training and evaluation datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Step 4: Define the data collator (automatically handles padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define the data collator (automatically handles padding)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Set to False for causal language modeling (like GPT-2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Step 5: Define training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tju/Workspace/llm-hg-wikitext/.venv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Step 6: Set up the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,  # Add the evaluation dataset here\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Step 7: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Step 8:  Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4840741193794220aef43a2429c17d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.635511636734009, 'eval_runtime': 67.9022, 'eval_samples_per_second': 55.374, 'eval_steps_per_second': 27.687, 'epoch': 0.01}\n",
      "Evaluation results: {'eval_loss': 3.635511636734009, 'eval_runtime': 67.9022, 'eval_samples_per_second': 55.374, 'eval_steps_per_second': 27.687, 'epoch': 0.005337981371534397}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Step 9: Save the model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_llama2/tokenizer_config.json',\n",
       " './fine_tuned_llama2/special_tokens_map.json',\n",
       " './fine_tuned_llama2/vocab.json',\n",
       " './fine_tuned_llama2/merges.txt',\n",
       " './fine_tuned_llama2/added_tokens.json',\n",
       " './fine_tuned_llama2/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./fine_tuned_llama2\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Step 10: Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'In a legal context, it is essential to consider the circumstances under which an individual was convicted in Australia at the time of the offence. We refer to these as the first sentence for each prisoner being charged with an offence under section 5 of the Crimes Act 1989. The following are a few of the more relevant sections of this legislation from the beginning as well as an important statement from the Victorian Government concerning the nature of sentencing.\\n\\n\"As a matter of law a conviction of an offence shall not exceed'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline(\"text-generation\", model=\"./fine_tuned_llama2\", tokenizer=tokenizer)\n",
    "prompt = \"In a legal context, it is essential to\"\n",
    "generated_text = text_generator(prompt, max_length=100, num_return_sequences=1)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Step 11: Deploy the Model to a Production Environment, upload this mode to huggingface hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.token='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'\n",
    "api.upload_folder(folder_path=\"./fine_tuned_llama2\", repo_id=\"xju3/learning-wikitext-2-raw-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Step 12: Accessing the Model via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load model directly from Hugging Face Hub\n",
    "model_name = \"xju3/learning-wikitext-2-raw-1\"\n",
    "text_generator = pipeline(\"text-generation\", model=model_name, token=\"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\")\n",
    "\n",
    "# Generate text\n",
    "prompt = \"In a legal context, it is essential to\"\n",
    "result = text_generator(prompt, max_length=100, num_return_sequences=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Step 13: Test The Api Running on Local Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': \"In a legal context, it is essential to remember both the legal system's important role in enabling its construction, and its role as a source of knowledge to facilitate its continued growth.\\n\\nThe history of legal frameworks\\n\\nThe early forms of juridical authority have often varied. Some have been loosely based on Greek law; many have been incorporated into modern Greek law.\\n\\nLaw at the beginning of the eighteenth century was an integral part of Greek law and of politics generally (see: Herod\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:8000/generate\"\n",
    "# url = \"http://127.0.0.1:5000/generate\"\n",
    "data = {\"prompt\": \"In a legal context, it is essential to\", \"max_length\": 100}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
